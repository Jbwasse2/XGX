from typing import Optional, Tuple

import torch
from habitat.utils import profiling_wrapper
from habitat_baselines.common.rollout_storage import RolloutStorage
from habitat_baselines.rl.ddppo.algo.ddppo import DecentralizedDistributedMixin
from habitat_baselines.rl.ppo.policy import NetPolicy
from torch import Tensor
from torch import nn as nn
from torch import optim as optim

EPS_PPO = 1e-5


class PPO(nn.Module):
    def __init__(
        self,
        actor_critic: NetPolicy,
        clip_param: float,
        ppo_epoch: int,
        num_mini_batch: int,
        value_loss_coef: float,
        entropy_coef: float,
        lr: Optional[float] = None,
        eps: Optional[float] = None,
        max_grad_norm: Optional[float] = None,
        use_clipped_value_loss: bool = True,
        use_normalized_advantage: bool = True,
        finetune: bool = True,
    ) -> None:
        super().__init__()

        self.actor_critic = actor_critic

        self.clip_param = clip_param
        self.ppo_epoch = ppo_epoch
        self.num_mini_batch = num_mini_batch

        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef

        self.max_grad_norm = max_grad_norm
        self.use_clipped_value_loss = use_clipped_value_loss

        if not finetune:
            self.optimizer = optim.Adam(
                list(
                    filter(lambda p: p.requires_grad,
                           actor_critic.parameters())),
                lr=lr,
                eps=eps,
            )
        else:
            self.optimizer = optim.Adam([
                {
                    "params":
                    list(
                        filter(
                            lambda p: p.requires_grad,
                            actor_critic.critic.parameters(),
                        )),
                    "lr":
                    lr,
                    "eps":
                    eps,
                },
                {
                    "params":
                    list(actor_critic.net.state_encoder.parameters()),
                    "lr": 0.0,
                    "eps": eps,
                },
                {
                    "params":
                    list(actor_critic.action_distribution.parameters()),
                    "lr": 0.0,
                    "eps": eps,
                },
            ])
        self.device = next(actor_critic.parameters()).device
        self.use_normalized_advantage = use_normalized_advantage

    def forward(self, *x):
        raise NotImplementedError

    def get_advantages(self, rollouts: RolloutStorage) -> Tensor:
        advantages = (
            rollouts.buffers["returns"][:-1]  # type: ignore
            - rollouts.buffers["value_preds"][:-1])
        if not self.use_normalized_advantage:
            return advantages

        return (advantages - advantages.mean()) / (advantages.std() + EPS_PPO)

    def update(self,
               rollouts: RolloutStorage,
               LMN_LOSS_IGNORE=False) -> Tuple[float, float, float]:
        advantages = self.get_advantages(rollouts)
        pu.db

        value_loss_epoch = 0.0
        action_loss_epoch = 0.0
        dist_entropy_epoch = 0.0

        for _e in range(self.ppo_epoch):
            profiling_wrapper.range_push("PPO.update epoch")
            data_generator = rollouts.recurrent_generator(
                advantages, self.num_mini_batch)

            for batch in data_generator:
                (
                    values,
                    action_log_probs,
                    dist_entropy,
                    _,
                ) = self._evaluate_actions(
                    batch["observations"],
                    batch["recurrent_hidden_states"],
                    batch["prev_actions"],
                    batch["masks"],
                    batch["actions"],
                )

                ratio = torch.exp(action_log_probs - batch["action_log_probs"])
                surr1 = ratio * batch["advantages"]
                surr2 = (torch.clamp(ratio, 1.0 - self.clip_param, 1.0
                                     + self.clip_param) * batch["advantages"])

                if self.use_clipped_value_loss:
                    value_pred_clipped = batch["value_preds"] + (
                        values - batch["value_preds"]).clamp(
                            -self.clip_param, self.clip_param)
                    value_losses = (values - batch["returns"]).pow(2)
                    value_losses_clipped = (value_pred_clipped
                                            - batch["returns"]).pow(2)
                    value_loss = 0.5 * torch.max(value_losses,
                                                 value_losses_clipped)
                else:
                    value_loss = 0.5 * (batch["returns"] - values).pow(2)

                if LMN_LOSS_IGNORE:
                    mask = torch.where(batch["value_preds"] == -123)
                    if len(mask[0]) == batch["masks"].shape[0]:
                        continue
                    not_mask = torch.where(batch["value_preds"] != -123)
                    action_loss = -(torch.min(surr1, surr2)[not_mask].mean())
                    value_loss = value_loss[not_mask].mean()
                    dist_entropy = dist_entropy[not_mask[0]].mean()
                    # action_loss = -(torch.min(surr1, surr2))
                    # action_loss[not_mask] = 0.0
                    # value_loss[not_mask] = 0.0
                    # dist_entropy[not_mask[0]] = 0.0
                    # action_loss = action_loss.mean()
                    # value_loss = value_loss.mean()
                    # dist_entropy = dist_entropy.mean()

                else:
                    action_loss = -(torch.min(surr1, surr2).mean())
                    value_loss = value_loss.mean()
                    dist_entropy = dist_entropy.mean()

                self.optimizer.zero_grad()
                total_loss = (value_loss * self.value_loss_coef + action_loss
                              - dist_entropy * self.entropy_coef)
                self.before_backward(total_loss)
                total_loss.backward()
                self.after_backward(total_loss)

                self.before_step()
                self.optimizer.step()
                self.after_step()

                value_loss_epoch += value_loss.item()
                action_loss_epoch += action_loss.item()
                dist_entropy_epoch += dist_entropy.item()

            profiling_wrapper.range_pop()  # PPO.update epoch

        num_updates = self.ppo_epoch * self.num_mini_batch

        value_loss_epoch /= num_updates
        action_loss_epoch /= num_updates
        dist_entropy_epoch /= num_updates
        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch

    def _evaluate_actions(self, observations, rnn_hidden_states, prev_actions,
                          masks, action):
        r"""Internal method that calls Policy.evaluate_actions.  This is used instead of calling
        that directly so that that call can be overrided with inheritance
        """
        return self.actor_critic.evaluate_actions(observations,
                                                  rnn_hidden_states,
                                                  prev_actions, masks, action)

    def before_backward(self, loss: Tensor) -> None:
        pass

    def after_backward(self, loss: Tensor) -> None:
        pass

    def before_step(self) -> None:
        nn.utils.clip_grad_norm_(self.actor_critic.parameters(),
                                 self.max_grad_norm)

    def after_step(self) -> None:
        pass


class DDPPO(DecentralizedDistributedMixin, PPO):
    pass
